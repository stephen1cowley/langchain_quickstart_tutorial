{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"book.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClashEval : Quantifying the tug-of-war between an\n",
      "LLM’s internal prior and external evidence\n",
      "Kevin W\n",
      "{'source': 'book.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[0:100])\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "[999, 924, 938, 540, 907, 987, 933, 997, 504, 905, 996, 976, 992, 852, 980, 985, 956, 938, 1000, 999, 799, 975, 921, 957, 822, 996, 945, 979, 298, 969, 959, 883, 986, 958, 979, 970, 669, 950, 905, 948, 965, 537, 909, 914, 962, 344, 148, 529]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='ClashEval : Quantifying the tug-of-war between an\\nLLM’s internal prior and external evidence\\nKevin Wu*\\nDepartment of Biomedical Data Science\\nStanford University\\nStanford, CA 94305\\nkevinywu@stanford.eduEric Wu*\\nDepartment of Electrical Engineering\\nStanford University\\nStanford, CA 94305\\nwue@stanford.edu\\nJames Zou\\nDepartment of Biomedical Data Science\\nStanford University\\nStanford, CA 94305\\njamesz@stanford.edu\\nAbstract\\nRetrieval augmented generation (RAG) is frequently used to mitigate hallucinations\\nand provide up-to-date knowledge for large language models (LLMs). However,\\ngiven that document retrieval is an imprecise task and sometimes results in erro-\\nneous or even harmful content being presented in context, this raises the question of\\nhow LLMs handle retrieved information: If the provided content is incorrect, does\\nthe model know to ignore it, or does it recapitulate the error? Conversely, when the\\nmodel’s initial response is incorrect, does it always know to use the retrieved infor-', metadata={'source': 'book.pdf', 'page': 0}),\n",
       " Document(page_content='the model know to ignore it, or does it recapitulate the error? Conversely, when the\\nmodel’s initial response is incorrect, does it always know to use the retrieved infor-\\nmation to correct itself, or does it insist on its wrong prior response? To answer this,\\nwe curate a dataset of over 1200 questions across six domains (e.g., drug dosages,\\nOlympic records, locations) along with content relevant to answering each question.\\nWe further apply precise perturbations to the answers in the content that range\\nfrom subtle to blatant errors. We benchmark six top-performing LLMs, including\\nGPT-4o, on this dataset and find that LLMs are susceptible to adopting incorrect re-\\ntrieved content, overriding their own correct prior knowledge over 60% of the time.\\nHowever, the more unrealistic the retrieved content is (i.e. more deviated from\\ntruth), the less likely the model is to adopt it. Also, the less confident a model is in', metadata={'source': 'book.pdf', 'page': 0}),\n",
       " Document(page_content='However, the more unrealistic the retrieved content is (i.e. more deviated from\\ntruth), the less likely the model is to adopt it. Also, the less confident a model is in\\nits initial response (via measuring token probabilities), the more likely it is to adopt\\nthe information in the retrieved content. We exploit this finding and demonstrate\\nsimple methods for improving model accuracy where there is conflicting retrieved\\ncontent. Our results highlight a difficult task and benchmark for LLMs – namely,\\ntheir ability to correctly discern when it is wrong in light of correct retrieved content\\nand to reject cases when the provided content is incorrect. Our dataset, called\\nClashEval , and evaluations are open-sourced to allow for future benchmarking on\\ntop-performing models at https://github.com/kevinwu23/StanfordClashEval.\\n1 Introduction\\nLarge language models (LLMs) are prone to hallucinations and incorrect answers [Pal et al., 2023,', metadata={'source': 'book.pdf', 'page': 0}),\n",
       " Document(page_content='top-performing models at https://github.com/kevinwu23/StanfordClashEval.\\n1 Introduction\\nLarge language models (LLMs) are prone to hallucinations and incorrect answers [Pal et al., 2023,\\nSun et al., 2024, Ahmad et al., 2023]. Additionally, they are constrained to knowledge contained\\nin their training corpus and are unable to answer queries about recent events or publicly restricted\\ninformation. Retrieval augmented generation (RAG) is a commonly used framework that provides\\nPreprint. Under review.arXiv:2404.10198v2  [cs.CL]  10 Jun 2024', metadata={'source': 'book.pdf', 'page': 0}),\n",
       " Document(page_content='Figure 1: A schematic of generating modified documents for each dataset. A question is posed to\\nthe LLM with and without a reference document containing information relevant to the query. This\\ndocument is then perturbed to contain modified information and given as context to the LLM. We\\nthen observe whether the LLM prefers the modified information or its own prior answer.\\nrelevant retrieved content in the LLM prompt and can significantly improve model accuracy [Mao\\net al., 2020, Chen et al., 2024a, Lewis et al., 2020].\\nMost commercial LLMs, like ChatGPT [OpenAI, 2023], Gemini [Gemini Team, 2023], and Perplex-\\nity.ai, already employ RAG in their Web interfaces. For example, ChatGPT employs a Bing search,\\nwhereas Gemini accesses Google Search results. While this can greatly enhance the model’s ability\\nto answer questions, it also raises concern for when the retrieved documents or webpages contain', metadata={'source': 'book.pdf', 'page': 1}),\n",
       " Document(page_content='whereas Gemini accesses Google Search results. While this can greatly enhance the model’s ability\\nto answer questions, it also raises concern for when the retrieved documents or webpages contain\\nincorrect or harmful information [Dash et al., 2023, Daws, 2020, Nastasi et al., 2023]. Indeed, exam-\\nples of this behavior have already surfaced in widely deployed LLMs. For example, recent headlines\\nshowed Google’s AI Summary recommending people to \"eat rocks\" or \"put glue on their pizza\" [Hart,\\n2024, Williams, 2024], presumably due to erroneous or satirical webpages being retrieved. While\\nstricter document filtering or improved retrieval may help reduce this occurrence, it by no means is\\na cure-all against this problem. At its core, LLMs should not blindly repeat information presented\\nin context but should be able to arbitrate when external information conflicts with its own internal\\nknowledge. While the aforementioned example is one in which the retrieved document is the source', metadata={'source': 'book.pdf', 'page': 1}),\n",
       " Document(page_content='in context but should be able to arbitrate when external information conflicts with its own internal\\nknowledge. While the aforementioned example is one in which the retrieved document is the source\\nof error, the converse is also a significant problem: when the LLM insists on its own incorrect prior\\nanswer despite correct external information.\\nSome studies have previously investigated the nature of this tension between a model’s internal prior\\nknowledge and contextual information. Longpre et al. [2021] found that LLMs exhibited a strong\\npreference for information in the training data even when facts in the context were substituted with\\nsimilar but incorrect information. More recently, Xie et al. [2023] showed that models can either\\nbe highly susceptible to context or very biased towards its priors depending on how the context\\nis framed. Our study extends these works in two important ways. First, we present a dataset that', metadata={'source': 'book.pdf', 'page': 1}),\n",
       " Document(page_content='be highly susceptible to context or very biased towards its priors depending on how the context\\nis framed. Our study extends these works in two important ways. First, we present a dataset that\\ncontains examples not only when the context is wrong and the model is right but the converse (where\\nthe context is right but the model is wrong). This is important since a dataset that only measures the\\nLLM’s ability to reject wrong context can trivially excel at this task by simply always ignoring the\\ncontext. Instead, our dataset uniquely tests the LLM’s ability to arbitrate between its own parametric\\nknowledge and the contextual information to determine the most accurate response. Second, we\\nelicit a quantitative relationship between the LLM’s preference of prior or context and two important\\nvariables: (1) the model’s confidence in its prior response (via measuring the token probabilities of\\nthe initial response), and (2) the degree to which the contextual information provided deviates from', metadata={'source': 'book.pdf', 'page': 1}),\n",
       " Document(page_content='the initial response), and (2) the degree to which the contextual information provided deviates from\\nthe reference answer. Measuring these two dynamics is important for understanding how models\\ntransition between choosing the prior and the context and their inherent biases towards their priors or\\nthe context.\\nOur contributions\\n•We introduce ClashEval , a question-answering benchmark dataset of over 1200 questions\\nspanning six domains that include the relevant contextual document for answering each\\n2', metadata={'source': 'book.pdf', 'page': 1}),\n",
       " Document(page_content='question. The answer in each document is perturbed across a range of erroneous values,\\nfrom subtle to extreme.\\n•We benchmark six top-performing LLMs (GPT-4o, GPT-3.5, Llama-3-8b-instruct, Gemini\\n1.5, Claude Opus, and Claude Sonnet) on this dataset and report three relevant metrics.\\n•We provide a systematic analysis of context preference rates across three models on (1)\\nvarying degrees of perturbation on the contextual information and (2) the token probabilities\\nof the prior responses.\\n•We propose a simple way to improve performance on ClashEval by incorporating token\\nprobabilities.\\n2 Related Works\\nThe issue of hallucination in LLMs has been explored in multiple contexts and models [Ji et al., 2023,\\nKaddour et al., 2023]. As a response, RAG systems have been shown to reduce hallucination [Shuster\\net al., 2021, Kang et al., 2023]. Previous works have explored automated RAG evaluation frameworks', metadata={'source': 'book.pdf', 'page': 2}),\n",
       " Document(page_content='Kaddour et al., 2023]. As a response, RAG systems have been shown to reduce hallucination [Shuster\\net al., 2021, Kang et al., 2023]. Previous works have explored automated RAG evaluation frameworks\\nin various settings [Es et al., 2023a, Hoshi et al., 2023, Saad-Falcon et al., 2023a, Zhang et al., 2024].\\nFor example, some studies use LLMs to evaluate the faithfulness, answer relevance, and context\\nrelevance of RAG systems by using GPT-3.5 as an evaluator [Es et al., 2023b, Saad-Falcon et al.,\\n2023b]. In another study, the authors propose metrics such as noise robustness, negative rejection,\\ninformation integration, and counterfactual robustness [Chen et al., 2024b]. Multiple studies have\\nshown that RAG can mislead LLMs in the presence of complex or misleading search results and that\\nsuch models can still make mistakes even when given the correct response [Foulds et al., 2024, Shuster\\net al., 2021]. In relation to understanding model priors, other works have used log probabilities to', metadata={'source': 'book.pdf', 'page': 2}),\n",
       " Document(page_content='et al., 2021]. In relation to understanding model priors, other works have used log probabilities to\\nassess the LLM’s confidence in responses [Mitchell et al., 2023, Zhao et al., 2024]. However, so far\\nthere has not been a systematic exploration of a model’s confidence (via logprobs) and the model’s\\npreference for RAG-provided information. Previous work has also focused on ways to address model\\nadherence to incorrect context. For example, Longpre et al. [2021] suggests pretraining on substituted\\nfacts to improve future robustness and Xiang et al. [2024] proposes ensembling isolated answers\\nacross multiple documents. In this work, we focus on the case where LLMs are available only via\\ninference, and only one document is being used as context.\\n3 Methods\\n3.1 Definitions and Metrics\\nFollowing the notation from Longpre et al. [2021], Xie et al. [2023], we start with a QA instance\\nx= (q, c)where qis the query and cis the context provided to answer the query. A model’s', metadata={'source': 'book.pdf', 'page': 2}),\n",
       " Document(page_content='Following the notation from Longpre et al. [2021], Xie et al. [2023], we start with a QA instance\\nx= (q, c)where qis the query and cis the context provided to answer the query. A model’s\\nprior response isr(q), where the model is asked to answer the question with only its parametric\\nknowledge. A model’s contextual response isr(q|c), where its response to the query is conditioned\\non the provided context.\\nIn our study, we define the following metrics:\\n•Accuracy = Pr\\x02\\nr(q|c)is right |cis right or r(q)is right\\x03\\n, the probability the model re-\\nsponds correctly given that either the context is right or the prior is right.\\n•Prior Bias = Pr\\x02\\nr(q|c)is wrong |cis right and r(q)is wrong\\x03\\n, the probability the model\\nuses its prior while the context is correct.\\n•Context Bias = Pr\\x02\\nr(q|c)is wrong |cis wrong and r(q)is right\\x03\\n, the probability the model\\nuses the context while the prior is correct.\\nOur main analysis consists of evaluating the RAG question-answering capabilities of six LLMs when', metadata={'source': 'book.pdf', 'page': 2}),\n",
       " Document(page_content=', the probability the model\\nuses the context while the prior is correct.\\nOur main analysis consists of evaluating the RAG question-answering capabilities of six LLMs when\\nintroducing varying levels of perturbations on the RAG documents. For this study, our dataset consists\\nof 1,294 total questions across 6 different domains. We evaluate the following models: GPT-4o,\\nGPT3.5 (gpt-3.5-turbo-0125 ), Llama-3 ( Llama-3-7B-Instruct ),Claude Opus ,Claude Sonnet , and\\nGemini 1.5 Flash . For our contextual responses, we use a standard prompt template that is based on\\nRAG prompts used on popular LLM open-source libraries, with over 800k downloads as of March\\n2024 (LangChain and LlamaIndex). In addition to this standard prompt, we experiment with \"strict\"\\nand \"loose\" prompts, with results in 6. Full prompts used are provided in our GitHub repository.\\n3', metadata={'source': 'book.pdf', 'page': 2}),\n",
       " Document(page_content='3.2 Dataset\\nDataset Name # Questions # Perturbations Example Question\\nDrug Dosage 249 10 What is the maximum daily dosage in mg\\nfor extended release oxybutynin in adults\\nwith overactive bladder?\\nNews 238 10 How many points did Paige Bueckers score\\nin the Big East Tournament title game on\\nMarch 6, 2023?\\nWikipedia Dates 200 10 In which year was the census conducted that\\nreported the population of Lukhi village in\\nIran as 35, in 8 families?\\nSports Records 191 10 What is the Olympic record for Men’s 100\\nmetres in athletics (time)?\\nNames 200 3 Which former United States Senator, born\\nin 1955, also shares the surname with other\\nsenators at the state level in Wisconsin, Min-\\nnesota, Massachusetts, Puerto Rico, and\\nNew York City?\\nLocations 200 3 What is the name of the hamlet in Canada\\nthat shares its name with a Scottish sur-\\nname?\\nTable 1: Statistics for each dataset, including number of questions, number of perturbations applied\\nto each question, and an example question.', metadata={'source': 'book.pdf', 'page': 3}),\n",
       " Document(page_content='that shares its name with a Scottish sur-\\nname?\\nTable 1: Statistics for each dataset, including number of questions, number of perturbations applied\\nto each question, and an example question.\\nWe generate questions from six subject domains (summarized in 1. To generate a large set of question-\\nand-answer pairs, we extract a corpus of content webpages and then query GPT-4o to generate a\\nquestion based on the text, along with the ground truth answer and the excerpt used to generate the\\nquestion. Additionally, we select six different datasets to cover diverse knowledge domains and\\ndifficulties. For example, news articles are included as examples of out-of-distribution questions that\\ncannot be answered properly without context. For each dataset below, we provide the full prompts\\nused to generate questions in our GitHub repository. Generated questions significantly transform the\\noriginal data and are covered under fair use; full document content may be covered under copyright,', metadata={'source': 'book.pdf', 'page': 3}),\n",
       " Document(page_content='original data and are covered under fair use; full document content may be covered under copyright,\\nbut we provide the accompanying code to reproduce the data. As our data is sourced from the\\nAssociated Press and Wikipedia, there is no personally identifiable information or offensive content to\\nour knowledge. UpToDate contains drug information and does not contain PHI or offensive content.\\nDrug Dosages We initially randomly sampled 500 drug information pages from UpToDate.com, a\\nmedical reference website widely used by clinicians. To constrain the scope of questions, we specify\\nin the prompt that the answer must be numerical and in milligrams. To filter out generated questions\\nthat did not meet the specified criteria (e.g. ambiguous question, incorrect units, etc.), we perform an\\nadditional quality control step, where we ask GPT-4o to verify that the generated question fulfills all\\ncriteria. After this step, we have 249 question-answer pairs.', metadata={'source': 'book.pdf', 'page': 3}),\n",
       " Document(page_content='additional quality control step, where we ask GPT-4o to verify that the generated question fulfills all\\ncriteria. After this step, we have 249 question-answer pairs.\\nSports Records We pulled Olympic records pages from Wikipedia.org across 9 sports: athletics,\\nweightlifting, swimming, archery, track cycling, rowing, shooting, short-track speed skating, and\\nspeed skating. Records are extracted in a table format, from which questions are generated for each\\nrecord entry. In total, after filtering, we extracted 191 unique questions and answers.\\nNews Top headlines are pulled from the Associated Press RSS feed for dates ranging from 03/15/24\\nto 03/25/24. From an initial corpus of 1486 news articles, we use GPT-4o to generate one question per\\narticle, instructing it to produce questions for which there is a clear numerical answer. We performed\\nanother GPT-4o quality control step, which resulted in 238 unique question-answer pairs.\\n4', metadata={'source': 'book.pdf', 'page': 3}),\n",
       " Document(page_content='Figure 2: Examples from three datasets demonstrating differential LLM responses across various\\ntypes of context modifications. Responses in red indicate wrong responses (different than the answer);\\nresponses in green indicate correct responses.\\nDates, Names, and Cities We begin with a random sample of 1000 articles from Huggingface’s\\nWikipedia dataset (20220301.en, [Foundation]). We use GPT-4o to generate questions related to\\neach field (dates, names, and cities) and filter out responses where the excerpt is not exactly found in\\nthe context. To reduce ambiguity when matching groundtruth answers, we restrict the answers to\\nfit certain formats. For dates, we require that the answer adheres to a four-digit year (YYYY). For\\nnames, we require a first and last name (eg. George Washington). For cities, we remove any other\\nidentities (eg. Seattle, not Seattle, WA). For each domain, among the remaining question-answer\\npairs that fit these criteria, we randomly sample 200 for our evaluation set.', metadata={'source': 'book.pdf', 'page': 4}),\n",
       " Document(page_content='identities (eg. Seattle, not Seattle, WA). For each domain, among the remaining question-answer\\npairs that fit these criteria, we randomly sample 200 for our evaluation set.\\n3.3 Modifying the Retrieved Documents\\nWe perform systematic perturbations on each question/answer pair (as visualized in Figure 1. In three\\ndatasets with numerical answers (Drug Dosages, Sports Records, Latest News), we produce ten mod-\\nifications that act as multipliers on the original value: 0.1,0.2,0.4,0.8,1.2,1.5,2.0,3.0,5.0,10.0. In\\nthe Wikipedia Years dataset, we perform ten absolute modifications in increments of 20 years for\\na range of [−100,100]. For the Wikipedia Names and Locations, the discrete categories required\\nmore hand-crafted levels of variation. For each, we performed three categorical perturbations via\\nprompting: slight, significant, and comical. We provide the full prompts used in our study in our\\nGitHub repository. For example, for a name like Bob Green , a slight modification implies a small', metadata={'source': 'book.pdf', 'page': 4}),\n",
       " Document(page_content='prompting: slight, significant, and comical. We provide the full prompts used in our study in our\\nGitHub repository. For example, for a name like Bob Green , a slight modification implies a small\\ntweak to another real name ( Rob Greene ), whereas a significant modification produces a similar but\\nfictitious name ( Bilgorn Grevalle ), and a comical modification is an absurd variant ( Blob Lawnface ).\\nFor a city name like Miami , a slight modification changes the name of the most similar city ( Fort\\nLauderdale ), a significant modification produces a fictitious city name ( Marisole ), and a comical\\nmodification produces an absurd variant ( Miameme ). Because of differences in how each modified\\nfact might appear in the retrieved text, we utilize GPT-4o to generate the perturbed excerpts for\\n5', metadata={'source': 'book.pdf', 'page': 4}),\n",
       " Document(page_content='drug dosages and news. Each modified fact is replaced in the original retrieved text. Then, both the\\nquestion and context are posed to GPT-4, from which the answers, along with the log probabilities of\\nthe output tokens, are collected.\\n4 Results\\nModel Chosen Prior Correct Context Correct\\nClaude OpusPrior 0.585 (0.550, 0.619) 0.042 (0.027, 0.058)\\nContext 0.313 (0.282, 0.346) 0.901 (0.879, 0.923)\\nNeither 0.102 (0.082, 0.125) 0.057 (0.040, 0.075)\\nClaude SonnetPrior 0.436 (0.403, 0.469) 0.051 (0.037, 0.067)\\nContext 0.401 (0.374, 0.434) 0.881 (0.859, 0.903)\\nNeither 0.163 (0.138, 0.186) 0.068 (0.052, 0.086)\\nGemini 1.5Prior 0.388 (0.362, 0.416) 0.074 (0.058, 0.091)\\nContext 0.490 (0.461, 0.521) 0.860 (0.838, 0.881)\\nNeither 0.122 (0.103, 0.143) 0.066 (0.051, 0.082)\\nGPT-4oPrior 0.327 (0.293, 0.358) 0.041 (0.027, 0.056)\\nContext 0.608 (0.571, 0.643) 0.903 (0.881, 0.923)\\nNeither 0.065 (0.047, 0.083) 0.056 (0.040, 0.072)\\nGPT-3.5Prior 0.237 (0.213, 0.263) 0.057 (0.043, 0.072)', metadata={'source': 'book.pdf', 'page': 5}),\n",
       " Document(page_content='Context 0.608 (0.571, 0.643) 0.903 (0.881, 0.923)\\nNeither 0.065 (0.047, 0.083) 0.056 (0.040, 0.072)\\nGPT-3.5Prior 0.237 (0.213, 0.263) 0.057 (0.043, 0.072)\\nContext 0.626 (0.598, 0.657) 0.841 (0.817, 0.865)\\nNeither 0.137 (0.113, 0.160) 0.102 (0.082, 0.123)\\nLlama-3Prior 0.208 (0.185, 0.230) 0.041 (0.029, 0.054)\\nContext 0.529 (0.499, 0.558) 0.793 (0.767, 0.818)\\nNeither 0.263 (0.236, 0.291) 0.166 (0.145, 0.191)\\nTable 2: We report model behavior given a subset of the data where either the prior or the context is\\ncorrect. A model exhibits prior bias by choosing its prior when only the context is correct, while it\\nexhibits context bias by choosing the context when only the prior is correct. We also report when\\nneither the prior nor context answer is used in the model response.\\n4.1 Prior vs. Context Conflict Resolution\\nIn Table 2, Table 4, and Figure 5, we report the responses for each of the six models when only the', metadata={'source': 'book.pdf', 'page': 5}),\n",
       " Document(page_content='4.1 Prior vs. Context Conflict Resolution\\nIn Table 2, Table 4, and Figure 5, we report the responses for each of the six models when only the\\nprior is correct or only the context is correct. On one end, models like Llama-3 andGPT-3.5 are at\\nnear random accuracy at the task of discerning when to use the prior or context answer. On the other\\nhand, the top performing model on all three metrics is Claude Opus , with an accuracy of 74.3%, a\\ncontext bias of 15.7%, and a prior bias of 2.1%. Interestingly, while GPT-4o is the current highest\\nperforming model on LMSYS Chatbot Area (as of June 2024), it has a higher context bias than all\\nother models but GPT-3.5 . While Llama-3 has a lower context bias than GPT-4o , it also has a lower\\naccuracy because it has a higher rate of choosing neither the prior nor the context in its response.\\nExamples of questions and model responses are shown in 2.\\n4.2 Context Preference Rate vs. Degree of Context Modification', metadata={'source': 'book.pdf', 'page': 5}),\n",
       " Document(page_content='Examples of questions and model responses are shown in 2.\\n4.2 Context Preference Rate vs. Degree of Context Modification\\nWe consider the degree of deviation between the model’s prior response and the value contained in\\nthe retrieved context (Figure 3). After fitting a linear model over the data, we find a clear negative\\ncorrelation between the degree of modification in the context to the context preference rate. Models\\nthat perform stronger on ClashEval exhibit both a lower intercept and a more negative slope, indicating\\nhigher resistance to incorrect context. For example, Claude Opus adheres to incorrect contextual\\ninformation 30% less than GPT-4o for the same degrees of modification. Interestingly, these results\\nsuggest that each model has a different prior distribution over truthfulness across each domain.\\n6', metadata={'source': 'book.pdf', 'page': 5}),\n",
       " Document(page_content='Figure 3: We observe an inverse relationship between the context preference rate (y-axis) and the\\namount of deviation from the prior (x-axis). Each plot visualizes absolute deviation from the reference\\ninformation (for numerical datasets, up to two log-fold changes (along with the trendline); for \"Years\",\\nthe absolute number of years; for categorical datasets, a total of four modification categories) against\\ncontext preference rate.\\n4.3 Context Preference Rate vs. Prior Token Probability\\nIn Figure 4, we observe a consistent negative relationship between the token probability of the\\nmodel’s prior answer and the associated RAG preference rate for all six QA datasets. To visualize an\\neven distribution across probabilities, we bin the probabilities into ten equidistant bins in the range of\\n[0.0,1.0]. The slope indicates the effect of stronger model confidence on the model’s preference for\\nthe information presented in the retrieved context; we observe different slopes (ranging from -0.1', metadata={'source': 'book.pdf', 'page': 6}),\n",
       " Document(page_content='[0.0,1.0]. The slope indicates the effect of stronger model confidence on the model’s preference for\\nthe information presented in the retrieved context; we observe different slopes (ranging from -0.1\\nto -0.45), suggesting that the effectiveness of RAG in different QA domains can be characterized\\nas being relatively susceptible (e.g., with Dates questions) or robust (e.g., with News questions) to\\nthe model’s internal prior knowledge confidence. Specifically, a slope of -0.45, for instance, can\\nbe interpreted as expecting a 4.5% decrease in the likelihood of the LLM preferring the contextual\\ninformation for every 10% increase in the probability of the model’s prior response.\\n4.3.1 Initial Methods for Improving Prior vs. Context Conflict Resolution\\nBased on our observations from the relationship between the token probabilities and the rates of\\npreference for context, we posit that comparing token probabilities between r(q)andr(q|c)can', metadata={'source': 'book.pdf', 'page': 6}),\n",
       " Document(page_content='Based on our observations from the relationship between the token probabilities and the rates of\\npreference for context, we posit that comparing token probabilities between r(q)andr(q|c)can\\nimprove the abilities of models to resolve conflicts. In Table 3, Token Probability Correction is\\ndone by comparing the mean token probabilities of the model’s response with and without context.\\nIf the probability is higher for the prior than the contextual response, then we use the model’s\\ngeneration without context as its final response. Otherwise, we just use the response with context.\\nWe find that this method improves the overall accuracy of all three models with a moderate increase\\nin the prior bias of each model. Next, we observe that the probability distributions between prior\\nresponses and context-given responses are uncalibrated, where context-given response probabilities\\nare extremely right-tailed while prior probabilities are nearly uniform. As a simple adjustment, we', metadata={'source': 'book.pdf', 'page': 6}),\n",
       " Document(page_content='responses and context-given responses are uncalibrated, where context-given response probabilities\\nare extremely right-tailed while prior probabilities are nearly uniform. As a simple adjustment, we\\ncompare the percentiles rather than raw probability scores of each score, or the Calibrated Token\\n7', metadata={'source': 'book.pdf', 'page': 6}),\n",
       " Document(page_content='Figure 4: We additionally observe an inverse relationship between the context preference rate (y-\\naxis) and the model’s prior response probability (x-axis). Context preference rate is defined as the\\nproportion of responses that align with the information presented in the prompt as context. The\\nmodel’s prior response probability is computed from the average log probability of the response\\ntokens queried without context. Each plot visualizes the prior probability (grouped into 10 bins)\\nagainst the context preference rate, along with the best-fit trend line and slope. Models that allow\\naccess to token probabilities are shown.\\nModel Correction Accuracy ↑ Context Bias ↓ Prior Bias ↓\\nGPT-4oNo correction (Baseline) 0.615 (0.595, 0.636) 0.304 (0.287, 0.321) 0.021 (0.014, 0.028)\\nToken Probability Correction 0.693 (0.672, 0.714) 0.194 (0.177, 0.210) 0.043 (0.032, 0.053)\\nCalibrated Token Prob. Correction 0.754 (0.733, 0.775) 0.107 (0.093, 0.122) 0.085 (0.072, 0.098)', metadata={'source': 'book.pdf', 'page': 7}),\n",
       " Document(page_content='Token Probability Correction 0.693 (0.672, 0.714) 0.194 (0.177, 0.210) 0.043 (0.032, 0.053)\\nCalibrated Token Prob. Correction 0.754 (0.733, 0.775) 0.107 (0.093, 0.122) 0.085 (0.072, 0.098)\\nGPT-3.5No correction (Baseline) 0.539 (0.521, 0.557) 0.313 (0.298, 0.328) 0.028 (0.021, 0.036)\\nToken Probability Correction 0.596 (0.575, 0.616) 0.253 (0.237, 0.269) 0.056 (0.046, 0.067)\\nCalibrated Token Prob. Correction 0.701 (0.678, 0.722) 0.110 (0.098, 0.124) 0.147 (0.132, 0.164)\\nLlama-3No correction (Baseline) 0.500 (0.483, 0.515) 0.264 (0.250, 0.279) 0.021 (0.015, 0.027)\\nToken Probability Correction 0.556 (0.537, 0.574) 0.235 (0.220, 0.249) 0.046 (0.037, 0.055)\\nCalibrated Token Prob. Correction 0.649 (0.627, 0.669) 0.111 (0.099, 0.122) 0.188 (0.173, 0.204)\\nTable 3: For models which provide token probabilities, we evaluate the accuracy, context bias, and\\nprior bias under three conditions: (1) No correction, which is the baseline result from this paper, (2)', metadata={'source': 'book.pdf', 'page': 7}),\n",
       " Document(page_content='prior bias under three conditions: (1) No correction, which is the baseline result from this paper, (2)\\nthe token probability correction, and (3) the calibrated token probability correction.\\nProbability Correction . We find that calibrated token probability correction improves all models’\\noverall accuracy by 14% and context bias by 20%. At the same time, this introduces more prior bias,\\nfrom 2% to 8.5%. However, this method outperforms a baseline of randomly replacing the final\\nresponse with its prior – at the same bias rate of 8.5%, the random baseline has an accuracy of 57.5%\\nas compared to the 75.4% from the method. While this paper focuses on developing the ClashEval\\nbenchmark, these results suggest that probability calibration is a promising approach to reduce prior\\nand context bias deserving further investigation. It also is a natural baseline for future methods.\\n8', metadata={'source': 'book.pdf', 'page': 7}),\n",
       " Document(page_content='5 Discussion\\nTheClashEval benchmark dataset and evaluations provide novel insights into how LLMs arbitrate\\nbetween their own internal knowledge and contextual information when the two are in conflict.\\nA key finding is that even the most advanced LLMs like GPT-4o exhibit a strong context bias,\\noverriding their own correct prior knowledge over 60% of the time when presented with incorrect\\ninformation in the retrieved documents. However, this bias is not absolute - the degree to which\\nthe retrieved content deviates from truth negatively correlates with the context preference rate.\\nInterestingly, each LLM exhibits a different prior distribution over truthfulness across domains, such\\nthat the same perturbation level affects each model differently. For instance, for a given magnitude\\nof deviation, Claude Opus adheres to incorrect contextual information 30% less often than GPT-4o.\\nWhile GPT-4o achieves state-of-the-art results on general-purpose tasks, it exhibits higher context', metadata={'source': 'book.pdf', 'page': 8}),\n",
       " Document(page_content='of deviation, Claude Opus adheres to incorrect contextual information 30% less often than GPT-4o.\\nWhile GPT-4o achieves state-of-the-art results on general-purpose tasks, it exhibits higher context\\nbias compared to smaller models like Claude Sonnet. This finding suggests that performance on\\nknowledge-based benchmarks may not automatically mean it is most suitable for RAG settings.\\nAdditionally, we find that LLMs are calibrated to selectively defer to external evidence when they are\\nless certain about a given query. However, each model differs in how well-calibrated they are. While\\nstrong priors are not inherently problematic, the lack of explicit expectations around how models\\nwill decide to use contextual information remains a risk. We propose a simple method for improving\\nmodels under ClashEval , and hope that future work can improve upon this baseline.\\nOur analyses have several key limitations. First, RAG systems can be deployed to many more', metadata={'source': 'book.pdf', 'page': 8}),\n",
       " Document(page_content='models under ClashEval , and hope that future work can improve upon this baseline.\\nOur analyses have several key limitations. First, RAG systems can be deployed to many more\\ndomains than can be covered by our analyses. Second, to make our experiments tractable, our\\nquestion-generation process is strictly fact-based and does not require multi-step logic, document\\nsynthesis, or other higher-level reasoning. Third, our dataset contains an enriched rate of contextual\\nerrors, so the reported metrics are not meant to represent bias rates in the wild. Fourth, our proposed\\ntoken probability method only applies to models which provide probability outputs. Finally, even\\nthough this dataset is intended to improve an LLM’s ability to provide users with accurate information,\\nbad actors could use such information to exploit the shortcomings of certain models described in this\\npaper.\\nAs retrieval-augmented AI systems become increasingly prevalent, we hope our dataset and insights', metadata={'source': 'book.pdf', 'page': 8}),\n",
       " Document(page_content='paper.\\nAs retrieval-augmented AI systems become increasingly prevalent, we hope our dataset and insights\\nspur further research into improving the robustness and calibration of such models. Resolving the\\ntension between parametric priors and retrieved information is a crucial challenge on the path to safe\\nand trustworthy language models.\\nReferences\\nMuhammad Aurangzeb Ahmad, Ilker Yaramis, and Taposh Dutta Roy. Creating trustworthy LLMs:\\nDealing with hallucinations in healthcare AI. September 2023.\\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. Benchmarking large language models in\\nRetrieval-Augmented generation. AAAI , 38(16):17754–17762, March 2024a.\\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. Benchmarking large language models in\\nretrieval-augmented generation. In Proceedings of the AAAI Conference on Artificial Intelligence ,\\nvolume 38, pages 17754–17762, 2024b.\\nDebadutta Dash, Rahul Thapa, Juan M Banda, Akshay Swaminathan, Morgan Cheatham, Mehr', metadata={'source': 'book.pdf', 'page': 8}),\n",
       " Document(page_content='volume 38, pages 17754–17762, 2024b.\\nDebadutta Dash, Rahul Thapa, Juan M Banda, Akshay Swaminathan, Morgan Cheatham, Mehr\\nKashyap, Nikesh Kotecha, Jonathan H Chen, Saurabh Gombar, Lance Downing, Rachel Pedreira,\\nEthan Goh, Angel Arnaout, Garret Kenn Morris, Honor Magon, Matthew P Lungren, Eric Horvitz,\\nand Nigam H Shah. Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs\\nin healthcare delivery. April 2023.\\nRyan Daws. Medical chatbot using OpenAI’s GPT-3 told a fake patient to kill\\nthemselves. https://www.artificialintelligence-news.com/2020/10/28/\\nmedical-chatbot-openai-gpt3-patient-kill-themselves/ , October 2020. Accessed:\\n2024-1-19.\\n9', metadata={'source': 'book.pdf', 'page': 8}),\n",
       " Document(page_content='Shahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. RAGAS: Automated evaluation\\nof retrieval augmented generation. September 2023a.\\nShahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. Ragas: Automated evaluation\\nof retrieval augmented generation. arXiv preprint arXiv:2309.15217 , 2023b.\\nPhilip Feldman Foulds, R James, and Shimei Pan. Ragged edges: The double-edged sword of\\nretrieval-augmented chatbots. arXiv preprint arXiv:2403.01193 , 2024.\\nWikimedia Foundation. Wikimedia downloads. URL https://dumps.wikimedia.org .\\nGemini Team. Gemini: A family of highly capable multimodal models. December 2023.\\nRobert Hart. Google restricts ai search tool after “nonsensical” an-\\nswers told people to eat rocks and put glue on pizza, May 2024.\\nURL https://www.forbes.com/sites/roberthart/2024/05/31/\\ngoogle-restricts-ai-search-tool-after-nonsensical-answers-told-people-to-eat-rocks-and-put-glue-on-pizza/\\n?sh=64183b617f61 .', metadata={'source': 'book.pdf', 'page': 9}),\n",
       " Document(page_content='URL https://www.forbes.com/sites/roberthart/2024/05/31/\\ngoogle-restricts-ai-search-tool-after-nonsensical-answers-told-people-to-eat-rocks-and-put-glue-on-pizza/\\n?sh=64183b617f61 .\\nYasuto Hoshi, Daisuke Miyashita, Youyang Ng, Kento Tatsuno, Yasuhiro Morioka, Osamu Torii,\\nand Jun Deguchi. RaLLe: A framework for developing and evaluating Retrieval-Augmented large\\nlanguage models. August 2023.\\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang,\\nAndrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM\\nComputing Surveys , 55(12):1–38, 2023.\\nJean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert\\nMcHardy. Challenges and applications of large language models. arXiv preprint arXiv:2307.10169 ,\\n2023.\\nHaoqiang Kang, Juntong Ni, and Huaxiu Yao. Ever: Mitigating hallucination in large language', metadata={'source': 'book.pdf', 'page': 9}),\n",
       " Document(page_content='McHardy. Challenges and applications of large language models. arXiv preprint arXiv:2307.10169 ,\\n2023.\\nHaoqiang Kang, Juntong Ni, and Huaxiu Yao. Ever: Mitigating hallucination in large language\\nmodels through real-time verification and rectification. arXiv preprint arXiv:2311.09114 , 2023.\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\\nHeinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, and Others. Retrieval-augmented\\ngeneration for knowledge-intensive nlp tasks. Adv. Neural Inf. Process. Syst. , 33:9459–9474, 2020.\\nShayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh.\\nEntity-based knowledge conflicts in question answering. arXiv preprint arXiv:2109.05052 , 2021.\\nYuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, and Weizhu\\nChen. Generation-Augmented retrieval for open-domain question answering. September 2020.', metadata={'source': 'book.pdf', 'page': 9}),\n",
       " Document(page_content='Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, and Weizhu\\nChen. Generation-Augmented retrieval for open-domain question answering. September 2020.\\nE Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D Manning, and Chelsea Finn. De-\\ntectGPT: Zero-shot machine-generated text detection using probability curvature. ICML , pages\\n24950–24962, January 2023.\\nAnthony J Nastasi, Katherine R Courtright, Scott D Halpern, and Gary E Weissman. Does ChatGPT\\nprovide appropriate and equitable medical advice?: A vignette-based, clinical evaluation across\\ncare contexts. March 2023.\\nOpenAI. GPT-4 technical report. March 2023.\\nAnkit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Med-HALT: Medical domain\\nhallucination test for large language models. July 2023.\\nJon Saad-Falcon, Omar Khattab, Christopher Potts, and Matei Zaharia. ARES: An automated\\nevaluation framework for Retrieval-Augmented generation systems. November 2023a.', metadata={'source': 'book.pdf', 'page': 9}),\n",
       " Document(page_content='Jon Saad-Falcon, Omar Khattab, Christopher Potts, and Matei Zaharia. ARES: An automated\\nevaluation framework for Retrieval-Augmented generation systems. November 2023a.\\nJon Saad-Falcon, Omar Khattab, Christopher Potts, and Matei Zaharia. Ares: An automated evaluation\\nframework for retrieval-augmented generation systems. arXiv preprint arXiv:2311.09476 , 2023b.\\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval augmentation\\nreduces hallucination in conversation. arXiv preprint arXiv:2104.07567 , 2021.\\n10', metadata={'source': 'book.pdf', 'page': 9}),\n",
       " Document(page_content='Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan\\nLyu, Yixuan Zhang, Xiner Li, Zhengliang Liu, Yixin Liu, Yijue Wang, Zhikun Zhang, Bhavya\\nKailkhura, Caiming Xiong, Chaowei Xiao, Chunyuan Li, Eric Xing, Furong Huang, Hao Liu, Heng\\nJi, Hongyi Wang, Huan Zhang, Huaxiu Yao, Manolis Kellis, Marinka Zitnik, Meng Jiang, Mohit\\nBansal, James Zou, Jian Pei, Jian Liu, Jianfeng Gao, Jiawei Han, Jieyu Zhao, Jiliang Tang, Jindong\\nWang, John Mitchell, Kai Shu, Kaidi Xu, Kai-Wei Chang, Lifang He, Lifu Huang, Michael Backes,\\nNeil Zhenqiang Gong, Philip S Yu, Pin-Yu Chen, Quanquan Gu, Ran Xu, Rex Ying, Shuiwang\\nJi, Suman Jana, Tianlong Chen, Tianming Liu, Tianyi Zhou, Willian Wang, Xiang Li, Xiangliang\\nZhang, Xiao Wang, Xing Xie, Xun Chen, Xuyu Wang, Yan Liu, Yanfang Ye, Yinzhi Cao, Yong\\nChen, and Yue Zhao. TrustLLM: Trustworthiness in large language models. January 2024.', metadata={'source': 'book.pdf', 'page': 10}),\n",
       " Document(page_content='Zhang, Xiao Wang, Xing Xie, Xun Chen, Xuyu Wang, Yan Liu, Yanfang Ye, Yinzhi Cao, Yong\\nChen, and Yue Zhao. TrustLLM: Trustworthiness in large language models. January 2024.\\nRhiannon Williams. Why google’s AI overviews gets things wrong. MIT Technology Review , May\\n2024.\\nChong Xiang, Tong Wu, Zexuan Zhong, David Wagner, Danqi Chen, and Prateek Mittal. Certifiably\\nrobust rag against retrieval corruption. arXiv preprint arXiv:2405.15556 , 2024.\\nJian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and Yu Su. Adaptive chameleon or stubborn\\nsloth: Unraveling the behavior of large language models in knowledge conflicts. arXiv preprint\\narXiv:2305.13300 , 2023.\\nZihan Zhang, Meng Fang, and Ling Chen. RetrievalQA: Assessing adaptive Retrieval-Augmented\\ngeneration for short-form Open-Domain question answering. February 2024.\\nQinyu Zhao, Ming Xu, Kartik Gupta, Akshay Asthana, Liang Zheng, and Stephen Gould. The first to', metadata={'source': 'book.pdf', 'page': 10}),\n",
       " Document(page_content='generation for short-form Open-Domain question answering. February 2024.\\nQinyu Zhao, Ming Xu, Kartik Gupta, Akshay Asthana, Liang Zheng, and Stephen Gould. The first to\\nknow: How token distributions reveal hidden knowledge in large Vision-Language models? March\\n2024.\\nA Appendix\\nModel Context Bias ↓ Prior Bias ↓ Accuracy ↑\\nClaude Opus 0.157 (0.141, 0.174) 0.021 (0.014, 0.029) 0.743 (0.723, 0.763)\\nClaude Sonnet 0.201 (0.184, 0.215) 0.025 (0.018, 0.033) 0.658 (0.641, 0.678)\\nGemini 1.5 0.245 (0.231, 0.260) 0.037 (0.029, 0.046) 0.624 (0.607, 0.641)\\nGPT-4o 0.304 (0.287, 0.321) 0.021 (0.013, 0.028) 0.615 (0.594, 0.633)\\nGPT-3.5 0.313 (0.298, 0.329) 0.028 (0.021, 0.036) 0.539 (0.522, 0.558)\\nLlama-3 0.264 (0.250, 0.280) 0.021 (0.015, 0.027) 0.500 (0.482, 0.518)\\nTable 4: We compare six top-performing models across three metrics. Context bias is when the model\\nchooses the context answer when its prior was correct. Prior bias is when the model chooses its prior', metadata={'source': 'book.pdf', 'page': 10}),\n",
       " Document(page_content='chooses the context answer when its prior was correct. Prior bias is when the model chooses its prior\\nwhen the context answer is correct. Finally, accuracy is a straightforward measure of the fraction\\nof times it can produce the correct answer. We find that Claude Opus performs the best across all\\nmetrics with a context bias rate of 0.157.\\n11', metadata={'source': 'book.pdf', 'page': 10}),\n",
       " Document(page_content='Figure 5: We plot the data from Table 4 – each model’s performance across three metrics in different\\ncolors, along with 95% confidence intervals.\\n12', metadata={'source': 'book.pdf', 'page': 11}),\n",
       " Document(page_content='Figure 6: Effect of different prompts using GPT-4 on context preference rate vs prior probability. The\\n\"Strict\" prompt strongly enforces literal adherence to the retrieved context, while the \"Loose\" prompt\\nencourages the model to make a reasonable judgment in light of the provided context. We observe\\nlower and steeper drops in context preference with the loose vs strict prompts, suggesting that prompt\\nwording plays a significant factor in controlling context preference. Full prompts are provided in our\\nGitHub repository.\\n13', metadata={'source': 'book.pdf', 'page': 12})]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(len(splits))\n",
    "len(splits[-1].page_content)\n",
    "lengths = [len(x.page_content) for x in splits]\n",
    "print(lengths)\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Does GPT-4o have any bias?',\n",
       " 'context': [Document(page_content='5 Discussion\\nTheClashEval benchmark dataset and evaluations provide novel insights into how LLMs arbitrate\\nbetween their own internal knowledge and contextual information when the two are in conflict.\\nA key finding is that even the most advanced LLMs like GPT-4o exhibit a strong context bias,\\noverriding their own correct prior knowledge over 60% of the time when presented with incorrect\\ninformation in the retrieved documents. However, this bias is not absolute - the degree to which\\nthe retrieved content deviates from truth negatively correlates with the context preference rate.\\nInterestingly, each LLM exhibits a different prior distribution over truthfulness across domains, such\\nthat the same perturbation level affects each model differently. For instance, for a given magnitude\\nof deviation, Claude Opus adheres to incorrect contextual information 30% less often than GPT-4o.\\nWhile GPT-4o achieves state-of-the-art results on general-purpose tasks, it exhibits higher context', metadata={'page': 8, 'source': 'book.pdf'}),\n",
       "  Document(page_content='5 Discussion\\nTheClashEval benchmark dataset and evaluations provide novel insights into how LLMs arbitrate\\nbetween their own internal knowledge and contextual information when the two are in conflict.\\nA key finding is that even the most advanced LLMs like GPT-4o exhibit a strong context bias,\\noverriding their own correct prior knowledge over 60% of the time when presented with incorrect\\ninformation in the retrieved documents. However, this bias is not absolute - the degree to which\\nthe retrieved content deviates from truth negatively correlates with the context preference rate.\\nInterestingly, each LLM exhibits a different prior distribution over truthfulness across domains, such\\nthat the same perturbation level affects each model differently. For instance, for a given magnitude\\nof deviation, Claude Opus adheres to incorrect contextual information 30% less often than GPT-4o.\\nWhile GPT-4o achieves state-of-the-art results on general-purpose tasks, it exhibits higher context', metadata={'page': 8, 'source': 'book.pdf'}),\n",
       "  Document(page_content='5 Discussion\\nTheClashEval benchmark dataset and evaluations provide novel insights into how LLMs arbitrate\\nbetween their own internal knowledge and contextual information when the two are in conflict.\\nA key finding is that even the most advanced LLMs like GPT-4o exhibit a strong context bias,\\noverriding their own correct prior knowledge over 60% of the time when presented with incorrect\\ninformation in the retrieved documents. However, this bias is not absolute - the degree to which\\nthe retrieved content deviates from truth negatively correlates with the context preference rate.\\nInterestingly, each LLM exhibits a different prior distribution over truthfulness across domains, such\\nthat the same perturbation level affects each model differently. For instance, for a given magnitude\\nof deviation, Claude Opus adheres to incorrect contextual information 30% less often than GPT-4o.\\nWhile GPT-4o achieves state-of-the-art results on general-purpose tasks, it exhibits higher context', metadata={'page': 8, 'source': 'book.pdf'}),\n",
       "  Document(page_content='drug dosages and news. Each modified fact is replaced in the original retrieved text. Then, both the\\nquestion and context are posed to GPT-4, from which the answers, along with the log probabilities of\\nthe output tokens, are collected.\\n4 Results\\nModel Chosen Prior Correct Context Correct\\nClaude OpusPrior 0.585 (0.550, 0.619) 0.042 (0.027, 0.058)\\nContext 0.313 (0.282, 0.346) 0.901 (0.879, 0.923)\\nNeither 0.102 (0.082, 0.125) 0.057 (0.040, 0.075)\\nClaude SonnetPrior 0.436 (0.403, 0.469) 0.051 (0.037, 0.067)\\nContext 0.401 (0.374, 0.434) 0.881 (0.859, 0.903)\\nNeither 0.163 (0.138, 0.186) 0.068 (0.052, 0.086)\\nGemini 1.5Prior 0.388 (0.362, 0.416) 0.074 (0.058, 0.091)\\nContext 0.490 (0.461, 0.521) 0.860 (0.838, 0.881)\\nNeither 0.122 (0.103, 0.143) 0.066 (0.051, 0.082)\\nGPT-4oPrior 0.327 (0.293, 0.358) 0.041 (0.027, 0.056)\\nContext 0.608 (0.571, 0.643) 0.903 (0.881, 0.923)\\nNeither 0.065 (0.047, 0.083) 0.056 (0.040, 0.072)\\nGPT-3.5Prior 0.237 (0.213, 0.263) 0.057 (0.043, 0.072)', metadata={'page': 5, 'source': 'book.pdf'})],\n",
       " 'answer': 'Yes, GPT-4o exhibits a strong context bias, overriding its own correct prior knowledge over 60% of the time when presented with incorrect information in the retrieved documents. This bias is not absolute and decreases as the deviation from the truth in the retrieved content increases.'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "results = rag_chain.invoke({\"input\": \"Does GPT-4o have any bias?\"})\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Discussion\n",
      "TheClashEval benchmark dataset and evaluations provide novel insights into how LLMs arbitrate\n",
      "between their own internal knowledge and contextual information when the two are in conflict.\n",
      "A key finding is that even the most advanced LLMs like GPT-4o exhibit a strong context bias,\n",
      "overriding their own correct prior knowledge over 60% of the time when presented with incorrect\n",
      "information in the retrieved documents. However, this bias is not absolute - the degree to which\n",
      "the retrieved content deviates from truth negatively correlates with the context preference rate.\n",
      "Interestingly, each LLM exhibits a different prior distribution over truthfulness across domains, such\n",
      "that the same perturbation level affects each model differently. For instance, for a given magnitude\n",
      "of deviation, Claude Opus adheres to incorrect contextual information 30% less often than GPT-4o.\n",
      "While GPT-4o achieves state-of-the-art results on general-purpose tasks, it exhibits higher context\n"
     ]
    }
   ],
   "source": [
    "print(results[\"context\"][0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page': 8, 'source': 'book.pdf'}\n"
     ]
    }
   ],
   "source": [
    "print(results[\"context\"][0].metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
